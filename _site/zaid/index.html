<!DOCTYPE html>
<html>
    <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <link rel="stylesheet" type="text/css" href="/assets/css/materialize.css">
      <link rel="stylesheet" href="https://cdn.rawgit.com/konpa/devicon/df6431e323547add1b4cf45992913f15286456d3/devicon.min.css">
      <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
      <title>Safa Creations</title>
      <script type="text/javascript">
if (screen.width <= 699) {
document.location = "mobile.html";
}
</script>
    </head>


  <h2 style="text-align:center"> Latest Blog Posts </h2>
<p style="text-align:center;"><a class="waves-effect waves-light btn green darken-2" target="_blank" href="https://medium.com/@zaidalissa/"><i class="material-icons right">library_books</i>Read more at Medium</a></p>
<div class="fixed-action-btn horizontal">
  <a class="btn-floating btn-large waves-effect waves-default pulse" href="/">
    <i class="material-icons">arrow_back</i>
  </a>
</div>






<div class="row">
  <div class="col s10 offset-s1">


    <h4 style="text-align:center"><b>How to install and running Cloudera Docker Container on Ubuntu</b></h4>
    <p style="text-align:justify"><p>This tutorial will show how to install and configure version 5.7.0 of Cloudera Distribution Hadoop (CDH 5) on Ubuntu 16.04 host using Docker.</p><h4><strong>What’s CDH?</strong></h4><p>CDH (Cloudera’s Distribution Including Apache Hadoop) is the most complete, tested, and widely deployed distribution of Apache Hadoop. CDH is 100% open source and is the only Hadoop solution to offer batch processing, interactive SQL and interactive search as well as enterprise-grade continuous availability. More enterprises have downloaded CDH than all other distributions combined.</p><h4><strong>Why Docker?</strong></h4><p>Getting down to the nuts and bolts, Docker allows applications to be isolated into containers with instructions for exactly what they need to survive that can be easily ported from machine to machine. Virtual machines also allow the exact same thing. While Docker has a more simplified structure compared to both of these, the real area where it causes disruption is resource efficiency.</p><h3>Install Docker</h3><p>Installing docker is very easy. The choice here is Ubuntu 16.04, so before start with the installation takes into consideration the <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/#prerequisites">requirements</a> then follow this guide.</p><h4>Uninstall old versions</h4><p>Older versions of Docker were called docker or docker-engine. If these are installed, uninstall them:</p><pre>$ sudo apt-get remove docker docker-engine docker.io</pre><p>The contents of <em>/var/lib/docker</em>, including images, containers, volumes, and networks, are preserved. Check the content using these commands below.</p><pre>$ sudo ls /var/lib/docker</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/636/1*jvi5-vR-4yZyiuymI-5Rxw@2x.png" /><figcaption>Contents of a /var/lib/docker.</figcaption></figure><h4>Install using the repository</h4><p><strong>Set up the repo</strong></p><p>Update the apt package index, install packages to allow apt to use a repository over HTTPS, and add Docker’s official GPG key:</p><pre>$ sudo apt-get update<br>$ sudo apt-get install \<br>    apt-transport-https \<br>    ca-certificates \<br>    curl \<br>    software-properties-common<br>$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</pre><p>Verify that you now have the key with the fingerprint<em> 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C </em><strong><em>0EBF CD88</em></strong>, by searching for the last 8 characters of the fingerprint.</p><pre>$ sudo apt-key fingerprint 0EBFCD88</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/341/1*Bvz0wdzO-MvbVtocL1sT6Q@2x.png" /><figcaption>Searching for the last 8 characters of the fingerprint.</figcaption></figure><p>Use the following command to set up the stable repository. You always need the stable repository, even if you want to install builds from the edge or test repositories as well.</p><pre>$ sudo add-apt-repository \<br>   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \<br>   $(lsb_release -cs) \<br>   stable&quot;</pre><p><strong>Install Docker CE</strong></p><p>Update the apt package index, list the available versions in the repo, then select and install a version of Docker CE:</p><pre>$ sudo apt-get update<br>$ apt-cache madison docker-ce</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/670/1*NFIPbm37gWqbqKJBz-ZG9A@2x.png" /><figcaption>Choose a <em>specific version</em> of Docker CE.</figcaption></figure><p>Install a specific version by its fully qualified package name, for example, docker-ce=5:18.09.0~3-0~ubuntu-xenial, and verify that Docker CE is installed correctly by running the hello-world image.</p><pre>$ sudo apt-get install docker-ce=5:18.09.0~3-0~ubuntu-xenial<br>docker run hello-world</pre><p>The last command downloads a test image and runs it in a container. When the container runs, it prints an informational message and exits.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/371/1*DDa1GTlgmYRvt-50_M8cTw@2x.png" /><figcaption>Informational message to verify Docker installation.</figcaption></figure><h3>Importing the Cloudera QuickStart Image</h3><p>Before importing the image assure Docker is running, and type this into the terminal in the home directory <em>/home/your_name</em>. This will take a couple of minutes to complete because it’s a large file size so you can take a cup of tea or whatever you like.</p><pre>$ docker pull cloudera/quickstart:latest</pre><p>The next image is to check if everything working fine.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/364/1*1lnkyw6q31KN_TL6UhcSJQ@2x.png" /><figcaption>Information message when downloading the image.</figcaption></figure><h3>Running a Cloudera QuickStart Container</h3><p>To run a container using the image, you must know the name or hash of the image. If you followed the instructions above, the name could be <strong>cloudera/quickstart: latest.</strong> The hash is also printed in the terminal when you import, or you can look up the hashes of all imported images with:</p><pre>$ docker images</pre><p>Once you know the name or hash of the image, you can run it:</p><pre>docker run -m 4G --memory-reservation 2G --memory-swap 8G --hostname=quickstart.cloudera --privileged=true -t -i -v $(pwd):/zaid --publish-all=true -p8888 -p8088 cloudera/quickstart /usr/bin/docker-quickstart</pre><p>Basically, this command is telling docker to run an image with <strong>4GByte</strong> the maximum amount of memory the container can use, with <strong>2GByte</strong> as soft limit smaller than <strong>4GByte</strong> which is activated when Docker detects contention or low memory on the host machine, and <strong>8GByte</strong> the amount of memory this container is allowed to swap to disk. Privileged mode is required for HBase database, with option <strong>-i </strong>means interactive, option <strong>-t</strong> means to open it in the terminal, and option <strong>-v</strong> allows to share volumes with the container, so anything that we put in the home directory, will show up in the Docker container under the directory <em>/zaid</em> . We have to change this to the directory of our files. The option<strong> — publish-all=true</strong> opens up all the host ports to the docker ports, so you can access programs like the Hue in the port 8888 and YARN in the port 8088, and others programs.</p><p>Using the command below we can check if the deployment of the image working smoothly.</p><pre>docker ps -a</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/367/1*XKIiFU9W74YLMZGwcUi44w@2x.png" /><figcaption>Information message about container Id.</figcaption></figure><h3>Getting HUE and YARN to work</h3><p>We need to check if Hue and YARN are working in our docker machine, so we take the container Id from the information generated by the last command and we utilize these Id with the docker inspect command.</p><pre>docker inspect [CONTAINER ID]</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/422/1*uWFhqTB0izp8ahl97vbl5A@2x.png" /><figcaption>Network settings of docker image.</figcaption></figure><p>From the last image, we take into account that Hue is working on the port <strong>8888</strong> inside the docker machine, <strong>32768</strong> outside the docker machine which means on our localhost, and YARN <strong>8088</strong> inside, <strong>32769</strong> outside.</p><blockquote>These images below are our prove that HUE and YARN working as expected, so we put this line localhost:32768 in our browser for HUE and localhost: 32769 for YARN.</blockquote><h4>HUE</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/836/1*9Bjdp9nwwRfxVoQ5Gaq27w@2x.png" /><figcaption>Quick start page of HUE.</figcaption></figure><h4>YARN</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/831/1*uMOeYK8PmlPkjzA5FoGMaQ@2x.png" /><figcaption>YARN all applications page.</figcaption></figure><h3>Collect system information</h3><p>Docker CE defaults values is to use of the system&#39;s memory. So the minimum you should use is 4GB, The laptop for this guide only has 8GB, so we allocate 4GB to docker when its running.</p><p>Using the command below we can check that the laptop memory is <strong>8031140 kB.</strong></p><pre>sudo cat /proc/meminfo</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/188/1*fzXkoZhylEO1kyh5jCt68g@2x.png" /><figcaption>Information about my laptop memory.</figcaption></figure><p>And to see if we are running Docker CE with the minimum configuration we use this command.</p><pre>docker stats [CONTAINER ID]</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/295/1*L34Qjg4X5XYnSWBaaEga3Q@2x.png" /><figcaption>Live stream of container statistics.</figcaption></figure><p>Once we used the last command it showed a live stream of statistics when the memory usage of Docker image is between 1.77 GBytes and 4GBytes.</p><blockquote>Thanks for reading. If you enjoyed this article, feel free to hit that follow button to stay in touch.</blockquote><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b7c77f147e03" width="1" height="1"></p>
  </div>
</div>



<div class="row">
  <div class="col s10 offset-s1">


    <h4 style="text-align:center"><b>Precision And Recall</b></h4>
    <p style="text-align:justify"><figure><img alt="" src="https://cdn-images-1.medium.com/max/690/1*AY5DHNBWRdwyBatQw2EMxw@2x.png" /><figcaption><a href="https://opensourceconnections.com/blog/2016/03/30/search-precision-and-recall-by-example/">Illustration of documents and results in the search for apples</a></figcaption></figure><p>In pattern recognition, information retrieval and binary classification, <strong>precision</strong> (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while <strong>recall</strong> (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. Both precision and recall are therefore based on an understanding and measure of relevance.</p><blockquote>Let me put it this way imagine you have something that returns X results for your search query. Precision means how many of those X are actually relevant for your query. Recall means how many of all the relevant results for that query did you actually return.</blockquote><p>The terms <strong>true condition</strong> (positive outcome) and <strong>predicted condition</strong> (negative outcome) are used when discussing Confusion Matrices. This means that you need to understand the differences with Type I and Type II Errors.</p><ul><li>Type I Error (or False Positive) is a result that indicates that a given condition is present when it really is not present. For example, a drug test may come back positive even though the person tested has never had the drug. The reading of the test goes in the false positive box if the model predicts a yes and the true data shows no.</li><li>Type II Error (or False Negative) is a result that indicates that a given condition is not present when it really is present. For example, a drug test may come back negative, while the person is indeed taking drugs. Did the model predict a no and the true data differs and read yes? Sad, but that reading goes into the false negative box.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/901/1*_pKmlMlRClGYeq0NhKq4RQ@2x.png" /><figcaption>Recall image via <a href="https://chrisalbon.com/machine_learning/preprocessing_structured_data/rescale_a_feature/">Chris Albon</a></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/901/1*2w8grs-82ugfV0Ge32L_UQ@2x.png" /><figcaption>Precision image via <a href="https://chrisalbon.com/machine_learning/preprocessing_structured_data/rescale_a_feature/">Chris Albon</a></figcaption></figure><p><em>This is an example in Python to understand the idea behind Precision and Recall.</em></p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/eda36c0af5345bef4469d30baed48223/href">https://medium.com/media/eda36c0af5345bef4469d30baed48223/href</a></iframe><blockquote><em>Thanks for reading. If you loved this article, feel free to hit that follow button so we can stay in touch.</em></blockquote><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=846fa88f7df4" width="1" height="1"></p>
  </div>
</div>



<div class="row">
  <div class="col s10 offset-s1">


    <h4 style="text-align:center"><b>Bias-variance dilemma?</b></h4>
    <p style="text-align:justify"><p>The Bias-Variance dilemma is relevant for supervised machine learning. It’s a way to diagnose an algorithm performance by breaking down its prediction error. There are three types of prediction errors: bias, variance, and irreducible error.</p><ul><li><strong>Bias error</strong>: The error due to bias as the difference between the expected (or average) prediction of the model and the true value which is trying to predict. Of course, there is only one model so talking about expected or average prediction values might seem a little wired. However, if it repeats the model building process more than once: each time gathering new data and run a new analysis creating a new model. Due to randomness in the underlying data sets, the resulting models will have a range of predictions. Bias measures how far off, in general, these models’ predictions are from the correct value. Imagine fitting a linear regression to a dataset that has a non-linear pattern:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/590/1*-DN0BjgJgCW3UCqTZnNPtw@2x.png" /><figcaption>High bias model(underfitting).</figcaption></figure><p>No matter how many more observations are collected, a linear regression won’t be able to model the curves in that data! This is known as underfitting.</p><ul><li><strong>Variance error</strong>: The error due to variance is the variability of a model prediction for a given data point. Again, imagine it could be possible to repeat the entire model building process multiple times. The variance is how much the predictions for a given point vary between different realizations of the model. For example, there is an algorithm that fits a completely unconstrained, flexible model to the dataset.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*nXz6y4ovOIAsDqznNaBmZg@2x.png" /><figcaption>High variance (overfitting)</figcaption></figure><p>As seen in the figure above, this unconstrained model has basically memorized the training set, including all the noise. This is known as overfitting.</p><ul><li><strong>The irreducible error </strong>is the noise term in the true relationship that cannot fundamentally be reduced by any model. It typically comes from inherent randomness or an incomplete feature set.</li></ul><p>At its root, dealing with bias and variance is really about dealing with underfitting and overfitting. <strong>Bias</strong> is reduced and <strong>variance</strong> is increased in relation to model complexity. For example, as more polynomial terms are added to a linear regression, the greater the resulting model’s complexity will be. In other words, bias has a negative first-order derivative in response to model complexity while variance has a positive slope.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/425/1*nSpgb17nJWOUSzKzXI3ilw@2x.png" /><figcaption>The relation between bias and variance.</figcaption></figure><h4>Why there is a trade-off between bias and variance?</h4><p>Low variance (high bias) algorithms turn to be<strong> </strong>less complex, with simple or rigid underlying structure. These models include linear or parametric algorithms such as regression and naive Bayes.</p><p>On the other hand, low bias (high variance) algorithms turn to be more complex, with a flexible underlying structure. These models include non-linear or non-parametric algorithms such as decision trees and nearest neighbors.</p><p>This tradeoff in complexity is there’s a tradeoff in bias and variance an algorithm cannot simultaneously be more complex and less complex.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/418/1*On4Uk9Favg50ylBOak-ECQ@2x.png" /><figcaption><em>Understanding the Bias-Variance Tradeoff, by Scott Fortmann-Roe.</em></figcaption></figure><h4>What is the total error?</h4><p>The total error may then be decomposed into bias, variance, and irreducible error components:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/577/1*8Iwv1C9e9dUzQR5UuH-eAw.jpeg" /><figcaption>Bias-Variance card by Chris Albon.</figcaption></figure><h4>How should be possible to detect overfitting and underfitting and what solutions exist to solve it?</h4><p>Overfitting results in low training error and high test error, while underfitting results in high errors in both the training and test set.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/597/1*6MJ4qfvVJUMfbtU4hQPXfQ@2x.png" /></figure><p>However, measuring training and test errors is hard when there are relatively few data points and the algorithms need many data points. In this case, a good choice to use is a technique called <strong>cross-validation</strong>.</p><p>This is where we take our entire dataset and split it into k groups. For each of the k groups, we train on the remaining k−1 groups and validate on the kth group. This way we can make the most use of our data, essentially taking a dataset and training k times on it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*0G9CzbPadQaCXkxrTkqN7A@2x.png" /><figcaption>Cross-validation</figcaption></figure><p>As for what to do after you detect a problem? Well, having the high bias is symptomatic of a model that is simple enough. In that case, the best bet would be to just pick a more complex model(getting more features or try adding polynomial features)</p><p>The problem of high variance is a bit more interesting. One naive approach to reduce high variance is to use more data. Theoretically, with a complex model, as the number of samples tends toward infinity the variance tends toward zero. However, this approach is naive because the rate at which the variance decreases is typically fairly slow, and the large data problem is almost always very hard to come across.</p><p>A better solution to reduce variance is to use <strong>regularization</strong>. It models the training data well, penalizes it for growing too complex. Essentially, regularization injects bias into the model by telling it not to become too complex. Common regularization techniques include lasso or ridge regression, dropout for neural networks, and soft margin SVMs.</p><h3><em>Thanks for reading. If you enjoyed this article, feel free to hit that follow button to stay in touch.</em></h3><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=74e5f1f52b12" width="1" height="1"></p>
  </div>
</div>



<div class="row">
  <div class="col s10 offset-s1">


    <h4 style="text-align:center"><b>Maarten this is a great article man, congrats.</b></h4>
    <p style="text-align:justify"><p>Maarten this is a great article man, congrats.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6bcb5cae5785" width="1" height="1"></p>
  </div>
</div>



<div class="row">
  <div class="col s10 offset-s1">


    <h4 style="text-align:center"><b>Standardization VS Normalization</b></h4>
    <p style="text-align:justify"><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*74CwRQ2mMjswdWCz7O4KKg@2x.png" /><figcaption>Feature Scaling via<a href="https://stats.stackexchange.com/questions/111467/is-it-necessary-to-scale-the-target-value-in-addition-to-scaling-features-for-re"> StackExchange</a></figcaption></figure><h4>Standardization</h4><p>Standardization (or <strong>Z-score normalization</strong>) is the process of rescaling the features so that they’ll have the properties of a Gaussian distribution with</p><p><em>μ</em>=0 and σ=1</p><p>where <em>μ</em> is the mean and <em>σ</em> is the standard deviation from the mean; standard scores (also called <strong><em>z</em></strong> scores) of the samples are calculated as follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/140/1*w5nOX2X-62jGQ6_52nqmFA@2x.png" /></figure><h4>Normalization</h4><p>Normalization often also simply called <strong>Min-Max scaling </strong>basically shrinks the range of the data such that the range is fixed between 0 and 1 (or -1 to 1 if there are negative values). It works better for cases in which the standardization might not work so well. If the distribution is not Gaussian or the standard deviation is very small, the min-max scaler works better.</p><p>Normalization is typically done via the following equation:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/901/1*QjZVCDwW1TMFTWF3rubemQ@2x.png" /><figcaption>Min-Max Scaling image via <a href="https://chrisalbon.com/machine_learning/preprocessing_structured_data/rescale_a_feature/">Chris Albon</a></figcaption></figure><h4>Use Cases</h4><p>Some examples of algorithms where feature scaling is important are:</p><ul><li>K-nearest neighbors with a Euclidean distance measure if want all features to contribute equally.</li><li>Logistic regression, SVM, perceptrons, neural networks.</li><li>K-means.</li><li>Linear discriminant analysis, principal component analysis, kernel principal component analysis.</li></ul><p>Graphical-model based classifiers, such as Fisher LDA or Naive Bayes, as well as Decision trees and Tree-based ensemble methods such Random Forest are invariant to feature scaling, but still it might be a good idea to rescale the data.</p><h4><strong>Drawbacks</strong></h4><p>Normalizing the data is sensitive to outliers, so if there are outliers in the data set it is a bad practice. Standardization creates a new data not bounded (unlike normalization).</p><blockquote>This is a simple example in Python to understand how Standardization is working on Sonar dataset.</blockquote><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c75c2ae525247157bea5801be811b00c/href">https://medium.com/media/c75c2ae525247157bea5801be811b00c/href</a></iframe><h4>Conclusion</h4><p><em>Standardization and Normalization are created to achieve a similar target, which is to build features that have similar ranges to each other and widely used in data analysis to help the programmer to get some clue out of the raw data. In statistics, Standardization is the subtraction of the mean and then dividing by its standard deviation. In Algebra, Normalization is the process of dividing of a vector by its length and it transforms your data into a range between 0 and 1.</em></p><blockquote><em>Thanks for reading. If you loved this article, feel free to hit that follow button so we can stay in touch.</em></blockquote><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=da7a3a308c64" width="1" height="1"></p>
  </div>
</div>



<div class="row">
  <div class="col s10 offset-s1">


    <h4 style="text-align:center"><b>The Art of Writing Python: Yield and Generators</b></h4>
    <p style="text-align:justify"><p>Since the <em>yield</em> keyword is only used with generators, it makes sense to recall the concept of generators first and before generators come iterables.</p><h4>Iterables</h4><p>Everything one can use <em>for… in… </em>on is an iterable; lists, strings, files. These iterables are handy because someone can read them really easy, but all the values can be stored in memory and this is not always what the programmer wants when he has a lot of values.</p><pre>list_one = [1, 2, 3] <br>for element in list_one:<br>  print(element)<br>1<br>2<br>3</pre><pre>list_two = [2**x for x in range()]<br>for element in list_two:<br>  print(element)<br>1<br>2<br>4</pre><h4>Generators</h4><p>Generators are iterators, a kind of iterable someone can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly.</p><pre>the_generator = (x*x for x in range(3))<br>for element in the_generator:<br>  print(element)<br>0<br>1<br>4<br>one_generator = (x+x for x in range(3))<br>for element in one_generator:<br>  print(element)<br>0<br>2<br>4</pre><p>As easy as list comprehensions, except the <em>()</em> instead of <em>[]</em>. But, the computer scientist cannot perform<em> for</em> the element in one_generator a second time since generators can only be used once: they calculate 0, then forget about it then calculate 2, and end calculating 4, one by one.<br>Iterating over the list and the generator looks completely the same. However, although the generator is iterable, it is not a collection, and thus has no length. Collections (lists, tuples, sets) keep all values in memory and anyone can access them whenever needed. A generator calculates the values on the fly and think no more of them, so it does not have an overview about the own result set. Generators are especially useful for memory-intensive tasks, where there is no need to keep all of the elements in memory.</p><h4>Yield</h4><p><em>yield</em> is a keyword that is used like <em>return</em>, except the function will return a generator.</p><pre>def fibonacci(n):<br>     prev, current = 0, 1    <br>     counter = 0<br>     while counter &lt; n:<br>         yield current<br>         prev, current = current, prev + current<br>         counter += 1<br>fibo = fibonacci(7)<br>print(fibo)<br>&lt;generator object fibonacci at 0x7f537ae0ea40&gt;<br>for element in fibo:<br>     print(element)<br>1<br>1<br>2<br>3<br>5<br>8<br>13</pre><p>When the programmer calls the function, the code he has written in the function body does not run. The function only returns <em>generator object fibonacci at 0x7f537ae0ea40</em> . Then, the code will be run each time the for uses the generator.<br>The first time the for calls the generator object created from the function fibonacci, it will run the code in the function from the beginning until it hits yield, then it will return the first value of the loop which is 1. Then, each other call will run the loop that has been written in the function one more time, and return the next value, until there is no value to return.<br>The generator is considered empty once the function runs, but does not hit <em>yield</em> anymore. It can be because the loop had come to an end in which the counter reaches the value of n.</p><p>For more information check these two links <a href="https://www.python.org/dev/peps/pep-0255/">PEP-255</a> , <a href="https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do#">Yield</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b5a30b10095e" width="1" height="1"></p>
  </div>
</div>



<div class="row">
  <div class="col s10 offset-s1">


    <h4 style="text-align:center"><b>What is the Curse of Dimensionality?</b></h4>
    <p style="text-align:justify"><p>In machine learning dimensionality simply refers to the number of features(I.e. input variables in the datasets). when the number of features is very large relative to the number of observations in the dataset certain algorithms struggle to train efficient models.</p><blockquote>Let me put it this way imagine that you have a straight line of hundred meters long and you dropped a coin as Euro somewhere on it wouldn’t be too hard to find. You take some steps along the line and it takes you a couple of minutes. Now let’s say you have a square of one hundred meters and you dropped a Euro somewhere on it. It would be very hard like searching across one football field. It could take you a couple of days. Now imagine we have a cube of hundred meters across. That’s like a 30-storey building the size of a football stadium. The difficulty of searching through space gets a bit harder as you have more dimensions.</blockquote><p>Check this beautiful image of cats and dogs from this <a href="http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/">website</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/501/1*co8lvlGGtrYmLDqteORm6A@2x.png" /><figcaption>Curse Of Dimensionality</figcaption></figure><p><strong>How do you combat the curse of dimensionality?</strong></p><ol><li>Change the algorithm.</li><li>Reduce the dimensionality of the data.</li></ol><p><em>And in order to diminish the dimensionality a first solution is to apply Principal Component Analysis (PCA), and if not working, then other solutions would be:</em></p><ol><li>Feature selection algorithms.</li><li>Non-linear dimensionality reduction.</li><li>Feature hashing.</li><li>Clustering using K-Means.</li></ol><p><strong>Why you apply dimensionality reduction?</strong></p><ol><li>Increase in efficiency.</li><li>It helps in data compression and reduced storage space.</li><li>It reduces computation costs.</li><li>It also helps remove redundant features.</li><li>It fastens the time required for performing the same computations.</li><li>It improves the classification performance.</li><li>Ease of interpretation and modeling.</li></ol><p><strong>Why shouldn’t use apply dimensionality reduction?</strong></p><ul><li>Basically, it may lead to some amount of data loss.</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bc825ae6a421" width="1" height="1"></p>
  </div>
</div>



<div class="row">
  <div class="col s10 offset-s1">


    <h4 style="text-align:center"><b>Parametric vs Nonparametric models?</b></h4>
    <p style="text-align:justify"><p>There are two types of models, parametric and non-parametric, let’s start with parametric models.</p><p><strong>Parametric model</strong><br>A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples). No matter how much data you throw at a parametric model, it won’t change its mind about how many parameters it needs.</p><p>Some examples of parametric machine learning algorithms are:</p><ul><li>Linear Regression</li><li>Linear Support Vector Machines</li><li>Logistic Regression</li><li>Naive Bayes</li><li>Perceptron</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/400/1*P2oX7ZhTPsybXMWq9m5N1g@2x.png" /><figcaption>Parametric Model.</figcaption></figure><p><strong>Nonparametric models</strong><br>Nonparametric methods are good when you have a lot of data and no prior knowledge, and when you don’t want to worry too much about choosing just the right features.</p><p>Some examples of nonparametric models are:</p><ul><li>Decision Trees</li><li>K-Nearest Neighbor</li><li>Support Vector Machines with Gaussian Kernels</li><li>Artificial Neural Networks</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/306/1*gC4Cr1MkXBMRgJSbHwHOpw@2x.png" /><figcaption>Nonparametric Model</figcaption></figure><blockquote><strong>In conclusion</strong> with parametric models to predict new data, you only need to know the parameters of the model. In nonparametric methods are more flexible and for forecasting new data you need to know the parameters of the model and the state of the data that has been observed.</blockquote><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8bfa20726f4d" width="1" height="1"></p>
  </div>
</div>



<div class="row">
  <div class="col s10 offset-s1">


    <h4 style="text-align:center"><b>Data Science Interview Questions</b></h4>
    <p style="text-align:justify"><p><strong>Machine Learning Basics</strong></p><ol><li>What are parametric models? Give an example.</li><li>What is the “Curse of Dimensionality?”</li><li>How do you combat the curse of dimensionality?</li><li>Why you apply dimensionality reduction?</li><li>Explain dimensionality reduction, where it’s used, and its benefits?</li><li>Explain the Bias-Variance Tradeoff.</li><li>What is the trade-off between bias and variance?</li><li>What is selection bias, why is it important and how can you avoid it?</li><li>What is regularization, why do we use it, and give some examples of common methods?</li><li>What is data normalization and why do we need it?</li><li>What is data standardization (z-score normalization)?</li><li>How do you know which Machine Learning model you should use?</li><li>Explain what a false positive and a false negative are. Why is it important to differentiate these from each other?</li><li>Is it better to have too many false positives, or too many false negatives, Explain.</li><li>What’s the difference between Type I and Type II error?</li><li>What is a recommendation engine? How does it work?</li><li>What is Bayes’ Theorem? How is it useful in a machine learning context?</li><li>Hidden Markov models?</li><li>What’s your favorite algorithm, and can you explain it to me in less than a minute?</li><li>What’s the difference between a generative and discriminative model?</li><li>When should you use classification over regression?</li><li>Feature engineering, Categorical features, Numerical features?</li><li>Hyperparameters, parameters?</li><li>Information entropy, gini index, cross entropy?</li></ol><p><strong>Statistics</strong></p><ol><li>How would you explain a linear regression to a business executive?</li><li>What are some alternative models to a linear regression? Why are they better or worse?</li><li>What is the difference between “long” and “wide” format data?</li><li>How would you screen for outliers and what should you do if you find one?</li><li>What method do you use to determine whether the statistics published in an article (e.g. newspaper) are either wrong or presented to support the author’s point of view, rather than correct, comprehensive factual information on a specific subject?</li><li>What is statistical power?</li><li>Explain what resampling methods are and why they are useful. Also explain their limitations.</li><li>What’s the difference between probability and likelihood?</li><li>Intercept, Coefficient, Response, Predict?</li><li>Expected Value and Standard Deviation?</li><li>Mean, Variance, Standard deviation, Skewness, Kurtosis?</li><li>Pearson coefficient, Conditional number?</li><li>Covariance, Correlation and Confidence Intervals?</li><li>Random variables, probability distributions?</li><li>Cointegration and Stationarity</li><li>Probability distrbution fitting?</li><li>homoscedasticity?</li><li>What is the difference between Bayesian and frequentist statisticians?</li></ol><p><strong>Model Evaluation</strong></p><ol><li>What is the ROC Curve and what is AUC?</li><li>Why is Area Under ROC Curve (AUROC) better than raw accuracy as an out-of- sample evaluation metric?</li><li>How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression?</li><li>Explain what precision and recall are. How do they relate to the ROC curve?How can you prove that one improvement you’ve brought to an algorithm is really an improvement over not doing anything?</li><li>How would you evaluate a logistic regression model?</li><li>Which is more important to you– model accuracy, or model performance?</li><li>Confusion matrix, F1Score and Cohen’s Kappa?</li><li>Model selection, model evaluation and algorithm selection?</li><li>Probability calibration, sigmoid and isotonic calibration?</li></ol><p><strong>Data Preprocessing</strong></p><ol><li>How do you handle missing or corrupted data in a dataset?</li><li>How would you handle an imbalanced dataset?</li><li>What are 3 data preprocessing techniques to handle outliers?</li><li>What are 3 ways of reducing dimensionality?</li><li>How would you go about doing an Exploratory Data Analysis (EDA)?</li></ol><p><strong>Optimization</strong></p><ol><li>What is the difference between stochastic gradient descent (SGD) and gradient descent (GD)?</li><li>When would you use GD over SDG, and vice-versa?</li><li>What is gradient descent?</li><li>What is the Box-Cox transformation used for?</li><li>Manual, Grid search, Random search, Bayesian optimization, Evolutionary optimization?</li></ol><p><strong>Sampling &amp; Splitting</strong></p><ol><li>How much data should you allocate for your training, validation, and test sets?</li><li>If you split your data into train/test splits, is it still possible to overfit your model?</li><li>Explain over and underfitting and how to combat them?</li><li>What cross-validation technique would you use on a time series dataset?</li></ol><p><strong>Supervised Learning vs unsupervised learning</strong></p><ol><li>What is the difference between supervised and unsupervised machine learning?What are the advantages and disadvantages of decision trees?</li><li>What are the advantages and disadvantages of neural networks?</li><li>How can you choose a classifier based on training set size?</li><li>Explain Latent Dirichlet Allocation (LDA).</li><li>Explain Principal Component Analysis (PCA).</li><li>KPCA vs PCA?</li><li>t-SNE vs PCA?</li><li>How is KNN different from k-means clustering?</li><li>How can you prove that one improvement you’ve brought to an algorithm is really an improvement over not doing anything?</li><li>Factorization machines?</li></ol><p><strong>Kernel Methods</strong></p><ol><li>The kernel trick?</li><li>logistic regression vs SVC vs Perceptron?</li><li>SVM with RBF vs KNN?</li><li>Empirical risk minimization?</li><li>Hashing vector vs Tfidf vector?</li></ol><p><strong>Ensemble Learning</strong></p><ol><li>Why are ensemble methods superior to individual models?</li><li>Difference between bagging, boosting, and the relation to bayes theorem ?</li><li>Name an example where ensemble techniques might be useful?</li><li>Difference between bagging and random forest?</li><li>Bagging, boosting and stacking in machine learning?</li><li>XGBoost, AdaBoost?</li></ol><p><strong>Time Series Analysis</strong></p><ol><li>LSTM vs Kalman Filter?</li><li>Fourier transform vs Discrete cosine transform?</li><li>Discrete White Noise and Random Walks</li><li>Auto Regresssive Models AR(p)</li><li>Moving Average Models MA(q)</li><li>Auto Regresssive Moving Average Models ARMA(p,q)</li><li>Auto Regresssive Integrated Moving Average Models ARIMA(p,d,q)</li><li>Autoregressive Conditionally Heteroskedastic Models — ARCH(p)</li><li>Generalized Autoregressive Conditionally Heteroskedastic Models — GARCH(p,q)</li></ol><p><strong>Neural Networks</strong></p><ol><li>Activation Functions: Neural Networks Sigmoid, tanh, Softmax, ReLU, Leaky ReLU?</li><li>Autoencoders?</li><li>Batch gradient descent, Stochastic gradient descent, Mini-batch gradient descent?</li><li>Adaptive Learning Rate Methods={Momentum,Nesterov accelerated gradient, Adagrad, Adadelta, RMSprop, Adam, Nadam}</li><li>Batch size and memory of the GPU?</li><li>Clipping ANN?</li></ol><p><strong>Deep Learning</strong></p><ol><li>What is deep learning, and how does it contrast with other machine learning algorithms?</li><li>Side information, transfer knowledge?</li><li>What is max pooling in convolutional neural networks?</li><li>Why do we use convolutions for images rather than just Fully Connected layers?</li><li>What makes CNNs translation invariant?</li><li>Why do we have max-pooling in classification CNNs?</li><li>Why do segmentation CNNs typically have an encoder-decoder style / structure?</li><li>What is the significance of Residual Networks?</li><li>What is batch normalization and why does it work?</li><li>Why would you use many small convolutional kernels such as 3x3 rather than a few large ones?</li><li>Deep Learning Architectures?</li><li>Generative Adversarial Networks?</li></ol><p><strong>Reinforcement Learning</strong></p><ol><li>What is exploration vs exploitation?</li><li>What are the approximation methods in Reinforcement Learning?</li><li>Explain epsilon and exploration methods?</li><li>What are the value function, policy gradient, and actor-critic methods?</li><li>Difference between target vs online network?</li><li>Explain policy evaluation and policy improvement?</li><li>Explain experience replay and target network and why we use them?</li><li>Difference between reward and return?</li><li>Define each one of these concepts, action, state, observation, reward, value function, loss function, policy, agent, environment, history?</li></ol><p><strong>Recommender Systems</strong></p><ol><li>Collaborative filtering</li><li>Matrix factorization</li><li>Co-occurrence analysis</li><li>Content-based filtering</li><li>Graph-based algorithms</li><li>Hybrids</li><li>Deep learning</li><li>Self-actualization?</li><li>Spark ALS?</li></ol><p><strong>Visualization</strong></p><ol><li>Which tools do you use for visualization? What do you think of Tableau? R? SAS? (for graphs). How to efficiently represent 5 dimension in a chart (or in a video)?</li><li>Which data visualization libraries do you use? What are your thoughts on the best data visualization tools?</li></ol><p><strong>SQL</strong></p><ol><li>what is the difference between a left join and an inner join?</li><li>what are joins in SQL?</li></ol><p><strong>Programming</strong></p><ol><li>In pseudo-code or whatever language you would like: write a program that prints the numbers from 1 to 100. But for multiples of three print “Fizz” instead of the number and for the multiples of five print “Buzz”. For numbers which are multiples of both three and five print “FizzBuzz”.</li><li>What are some differences between a linked list and an array?</li><li>Describe a hash table.</li><li>Dynamic programming, greedy algorithms?</li><li>Big O notation?</li><li>Randomized algorithms?</li><li>Solving Recurrence?</li></ol><p><strong>The Art of Writing Python</strong></p><ol><li>Generators and Yield in Python?</li><li>Explain Series, Panel and DataFrame?</li><li>Basic Slicing and Indexing in Pandas?</li><li>Hierarchical Index?</li><li>What is the difference between Shallow Copy and Deep Copy?</li><li>What is the purpose of self?</li><li>Difference between Python 2 and Python3?</li><li>Difference between Pip vs Conda, create an environment in Conda?</li><li>NumPy arrays, Lists, Strings, Tuples, Dictionaries?</li><li>Unit testing and refactoring techniques?</li><li>Design patterns?</li></ol><p><strong>Pipelines In Python</strong></p><ol><li>Fit and Transform?</li><li>Types of pipelines?</li><li>Stateful vs stateless?</li><li>Huber loss vs log loss?</li><li>Extract Transform Load (ETL) data in python?</li></ol><p><strong>Spark and Streaming</strong></p><ol><li>RDD vs Random RDD?</li><li>Shuffle data workers?</li><li>Driver, Worker, Transformations, Actions?</li><li>Task, Closure, Accumulatores?</li><li>Lock, Unlock files in python?</li><li>Map, Flatmap, reduce?</li><li>Spark consensus, Spark Streaming linear regression?</li><li>Kafka, Yarn, Zookepeer, Flink, Flume, Spark ML, Spark MLIB, Kudu, Impala?</li></ol><p><strong>DevOps</strong></p><ol><li>AWS and Docker?</li><li>BASH scripts?</li><li>Jupyter notebook from AWS?</li></ol><p><strong>Business Specific</strong></p><ol><li>Do you have any other projects that would be related here?</li><li>Explain your current masters research? What worked? What didn’t? Future directions?</li><li>How can you help our marketing team be more efficient?</li><li>What is root cause analysis?</li><li>Are you familiar with pricing optimization, price elasticity, inventory management, competitive intelligence? Give examples.</li><li>Give an example of how you would use experimental design to answer a question about user behavior.</li><li>How would you implement a recommendation system for our company’s users?</li><li>How can we use your machine learning skills to generate revenue?</li></ol><p><strong>General Interest</strong></p><ol><li>How would you simulate the approach AlphaGo took to beat Lee Sidol at Go?</li><li>How do you think Google is training data for self-driving cars?</li><li>How would you approach the “Netflix Prize” competition?</li><li>What are your favorite use cases of machine learning models?</li><li>Do you have research experience in machine learning?</li><li>What are the last machine learning papers you’ve read?</li></ol><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=47990f14550f" width="1" height="1"></p>
  </div>
</div>


  <p style="text-align: center"><a class="waves-effect waves-light btn green darken-2" target="_blank" href="https://medium.com/@zaidalissa/"><i class="material-icons right">library_books</i>Read more at Medium</a></p>

</div>


<script type="text/javascript" src="/assets/js/jquery-3.2.1.min.js"></script>
<script type="text/javascript" src="/assets/js/materialize.min.js"></script>
<script src="/assets/js/init.js"></script>

</body>
<footer class="page-footer">
  <div class="footer-copyright" style="padding-left: 30px">
    <i class="material-icons" style="padding-right:5px">copyright</i><p>2017 Copyright Safa Creations LTD | All rights reserved</p>
  </div>
</footer>
</html>

